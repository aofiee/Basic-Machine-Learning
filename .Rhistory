test_df$dislikeCount[10:20]
#ยกกำลัง 2 เพื่อไม่ให้ error มีค่า ติดลบและถอดรูทเมื่อจบเพื่อหักล้างกัน
RMSE <- sqrt(mean((predicted_dislike - test_df$dislikeCount)**2))
my_model
RMSE
my_model
#เอา collumn ที่มีความสัมพันธ์กันมาเทรน
my_model <- train( dislikeCount ~ .,
data = train_df,
method = "knn",
trControl = ctrl,
tuneGrid = grid_search
)
my_model
#3. test model
predicted_dislike <- predict(my_model,newdata = test_df)
predicted_dislike[10:20]
test_df$dislikeCount[10:20]
confusionMatrix(predicted_dislike, test_df$dislikeCount , mode = "everything")
confusionMatrix(predicted_dislike, test_df$dislikeCount , mode = "everything")
grid_search <- expand.grid(k = c(1,3,5))
ctrl <- trainControl(method = "repeatedcv",
number = 15,
repeats = 15,
verboseIter = T
)
grid_search <- expand.grid(k = c(1,3,5))
ctrl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 100,
verboseIter = T
)
#เอา collumn ที่มีความสัมพันธ์กันมาเทรน
my_model <- train( dislikeCount ~ viewCount + likeCount + commentCount,
data = train_df,
method = "knn",
trControl = ctrl,
tuneGrid = grid_search
)
my_model
#3. test model
predicted_dislike <- predict(my_model,newdata = test_df)
predicted_dislike[10:20]
test_df$dislikeCount[10:20]
comment_content <- get_all_comments(video_id = 'iawt_JNv17g')
#youtube <- yt_oauth(
#  app_id = "15709576576-4c2uopcbiqg115nrdoclotrb7e7pjc56.apps.googleusercontent.com",
#  app_secret = "fIx3zhq6slSDR5-6G_qellfI"
#)
youtube
comment_content <- get_all_comments(video_id = 'iawt_JNv17g')
youtube <- yt_oauth(
app_id = "436728297200-kpvacab71dkrtmh8b2ihc7h7lqv82l4m.apps.googleusercontent.com",
app_secret = "zjzwjoARjv4JikRhMr9P0ve6"
)
comment_content <- get_all_comments(video_id = 'iawt_JNv17g')
comment_content <- get_all_comments(video_id = 'iawt_JNv17g')
saveRDS(my_model,file = "youtube.rds")
install.packages("Rfacebook", dependencies = TRUE)
library(Rfacebook)
token <- "EAAGn04jyM4sBAJCQZBOZCWJVOAIDIpfFxo6PcNSZBdJUDIZABe08nWMtj0bYe5yqkdHRf0g9vMRAEhSdq5B4x6MTZCtyOTPort1OiWJZAe8g6RdV3uAkQaTjf9pttm03amhlzEflyHLLUw3Y3ZCcqEmVHZAJGmq740dxu76hL0vMAh9ZBUUikdDxlApZBnrJLFXSdrOtEdLtZAzfwZDZD"
page <- getPage("TwinSynergyTH", token, n=100000)
library(Rfacebook)
token <- "EAAGn04jyM4sBAJCQZBOZCWJVOAIDIpfFxo6PcNSZBdJUDIZABe08nWMtj0bYe5yqkdHRf0g9vMRAEhSdq5B4x6MTZCtyOTPort1OiWJZAe8g6RdV3uAkQaTjf9pttm03amhlzEflyHLLUw3Y3ZCcqEmVHZAJGmq740dxu76hL0vMAh9ZBUUikdDxlApZBnrJLFXSdrOtEdLtZAzfwZDZD"
page <- getPage("TwinSynergyTH", token, n=100000)
library(Rfacebook)
token <- "EAAGn04jyM4sBAJCQZBOZCWJVOAIDIpfFxo6PcNSZBdJUDIZABe08nWMtj0bYe5yqkdHRf0g9vMRAEhSdq5B4x6MTZCtyOTPort1OiWJZAe8g6RdV3uAkQaTjf9pttm03amhlzEflyHLLUw3Y3ZCcqEmVHZAJGmq740dxu76hL0vMAh9ZBUUikdDxlApZBnrJLFXSdrOtEdLtZAzfwZDZD"
page <- getPage("TwinSynergyTH", token, n=1000)
page <- getPage("TwinSynergyTH", token, n=100)
page <- getPage("TwinSynergyTH", token, n=100)
page <- getPage("TwinSynergyTH", token, n=1)
token <- "EAAGn04jyM4sBAG2k13XFrhwrduIRVeXFS39cbrjuSioHPeN3mPyhRpWvpeBNgzFavd5ngJZAaN9I8K1bAfSjqZAeN4AX3yIffYnxmfEHRIn6CpFRX6OXTk3BJXMC0GSjFcih7LX3TZByQHuV9QCMwCkxUC54Bc9yg3NE2yuMwXcSZAAGmfWYNTxVdV2s5QaVA5ATrjAG9AZDZD"
page <- getPage("TwinSynergyTH", token, n=1)
page <- getPage("Twin Synergy", token, n=1)
page <- getPage("Twin Synergy", token, n=1)
page <- getPage("465477810183310", token, n=1)
install.packages("rvest")
install.packages("rvest")
library(rvest)
installed.packages("xml2")
library(rvest)
ig <- read_html(url)
url <- "https://www.instagram.com/"
ig <- read_html(url)
View(ig)
View(ig)
View(ig)
library(tidyverse)
library(rvest)
library(tidyverse)
url <- "https://www.instagram.com/"
ig <- read_html(url)
ig %>%
html_nodes('#react-root > section > main > section > div.cGcGK > div:nth-child(1) > div > article:nth-child(1) > div._97aPb > div > div > div > div.tN4sQ.zRsZI > div > div > div > ul > li:nth-child(1) > div > div > div > div._9AhH0') %>%
html_text()
ig %>%
html_nodes('#react-root > section > main > section > div.cGcGK > div:nth-child(1) > div > article:nth-child(1) > div.eo2As > div.KlCQn.EtaWk > ul > div > li > div > div > div') %>%
html_text()
ig %>%
html_nodes('#react-root > section > main > section > div.cGcGK > div:nth-child(1) > div > article:nth-child(1) > div.eo2As > div.KlCQn.EtaWk > ul > div > li > div > div > div') %>%
html_text()
library(tuber)
library(tidyverse)
library(caret)
#youtube <- yt_oauth(
#  app_id = "436728297200-kpvacab71dkrtmh8b2ihc7h7lqv82l4m.apps.googleusercontent.com",
#  app_secret = "zjzwjoARjv4JikRhMr9P0ve6"
#)
#youtube
#content_yt <- get_all_channel_video_stats(
#  channel_id = 'UC5FX2EmL4t7c305hfr-idwQ'
#)
#write.csv(content_yt,file = "cool.csv")
#content_yt
#ดูก่อนว่าได้มากี่ record
nrow(khotkool_youtube_content)
#สั่ง แสดง head ออกมาดู 6 อันว่ามีไรมั่ง
head(khotkool_youtube_content)
#มี feature อะไรให้เราเอามาเล่นบ้างล่ะ
names(khotkool_youtube_content)
glimpse(khotkool_youtube_content)
#เช็คข้อมูลกันก่อนว่ามี missing data ป่าววว
any(is.na(khotkool_youtube_content))
#พอดีเจอ missing data 1 record งั้นลบแม่ง
clean_data_khotkool <- na.omit(khotkool_youtube_content)
#ไหนๆ มีมั้ย เช็คอีกที
any(is.na(clean_data_khotkool))
#comment_content <- get_all_comments(video_id = 'iawt_JNv17g')
#tidyverse pipe จัดการชุดข้อมูลแยกตามรายการที่อยู่ใน โคตรคูล Channel กันทีละอัน
#เบื้องต้นคงเก็บไว้แค่ title viewCount likeCount dislikeCount commentCount url ตัดพวกวันที่ออก ไม่รู้จะเอาไปไร
clean_data_khotkool %>%
filter(str_detect(title,'หมีพาซิ่ง')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 0) -> mee_pa_zine
#clean หมีพาซิ่งแล้ว
head(mee_pa_zine)
#จัดการรายการวันละม้วนต่อ
clean_data_khotkool %>%
filter(str_detect(title,'วันละม้วน')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 1) -> one_la_muan
head(one_la_muan)
clean_data_khotkool %>%
filter(str_detect(title,'คนหน้าหมี') & !str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 2) -> khon_na_mee
#ตามด้วยนี่ รายการโปรด คนหน้าหมี
head(khon_na_mee)
clean_data_khotkool %>%
filter(str_detect(title,'แม่แยมเอง') & !str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 3) -> mae_yam_eang
#แม่แยมเอง อันนี้ไม่เคยดู ฮาๆๆ ไม่เป็นไร พัก data ไว้ก่อน
head(mae_yam_eang)
clean_data_khotkool %>%
filter(str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 4) -> vlog
#vlog ก็ไม่เคยดู พักไว้ๆ
head(vlog)
clean_data_khotkool %>%
filter(str_detect(title,'จีบหนูหน่อย') & !str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 5) -> jeep_nu_noi
#นี่รายการโปรดอีกตัว ฮาๆๆ คนมาออกรายการไม่น่าโสดนะแหม่
head(jeep_nu_noi)
#combine all content channel
all_content_channel <- rbind(mee_pa_zine,one_la_muan,jeep_nu_noi,vlog,mae_yam_eang,khon_na_mee)
#ลำดับรายการที่มียอดวิว มากที่สุดได้แก่รายการรรรรรรร หมีพาซิ่งครับ ซิ่งมียอดวิวมากถึง 30 กว่าล้านยอดวิว
#หมีพาซิ่งมี 18 คลิป
#vlog มี 32 คลิป
#จีบหนูหน่อย 17 คลิป
#คนหน้าหมี 27 คลิป
#แม่แยมเอง 21 คลิป
#วันละม้วน 21 คลิป
all_content_channel %>%
group_by(content_group) %>%
summarise(mostViews = sum(viewCount)) %>%
arrange(desc(mostViews)) %>%
ggplot(.,mapping = aes(
x=reorder(content_group,-mostViews),
y=mostViews/1000)) + geom_col(col = "blue", fill = "navy", alpha = 0.7) +
coord_flip() +
labs(x = "Content Name",
y = "Total Views x 1000")
#งั้นเอารายการหมีพาซิ่งมาดูกันว่าเทปไหน ฮิตสุด
mee_pa_zine %>%
arrange(desc(viewCount)) %>%
head(10) %>%
select(url,viewCount)  %>%
ggplot(.,mapping = aes(
x=reorder(url,-viewCount),
y=viewCount/100)) + geom_col(col = "blue", fill = "navy", alpha = 0.7) +
coord_flip() +
labs(x = "Content Name",
y = "Total Views x 100")
#หาความสัมพันธ์ระหว่าง dislike กับ ยอดวิวว่าสัมพันธ์กันไหม
cor(all_content_channel$dislikeCount,all_content_channel$viewCount)
#หาความสัมพันธ์ระหว่าง dislike กับ ยอดไลค์
cor(all_content_channel$dislikeCount,all_content_channel$likeCount)
#หาความสัมพันธ์ระหว่าง dislike กับ ยอดจำนวนคอมเม้น
cor(all_content_channel$dislikeCount,all_content_channel$commentCount)
#1.prepare data
set.seed(9)
# จำนวน Row
n <- nrow(all_content_channel)
#สุ่ม Data มา 70% จาก data ทั้งหมด
id <- sample(1:n, size = 0.7*n)
#สร้าง train data ที่ได้จาก id
train_df <- all_content_channel[id, ]
#สร้าง test data ที่ไม่อยู่ใน id ด้วย expression -id
test_df <- all_content_channel[-id,]
#ดูโครงสร้างของ data frame
str(all_content_channel)
set.seed(99)
grid_search <- expand.grid(k = c(1,3,5))
ctrl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 5,
verboseIter = T
)
#เอา collumn ที่มีความสัมพันธ์กันมาเทรน
my_model <- train( dislikeCount ~ viewCount + likeCount + commentCount,
data = train_df,
method = "knn",
trControl = ctrl,
tuneGrid = grid_search
)
my_model
#3. test model
predicted_dislike <- predict(my_model,newdata = test_df)
predicted_dislike[10:20]
test_df$dislikeCount[10:20]
RMSE <- sqrt(mean((predicted_dislike - test_df$dislikeCount)**2))
my_model
RMSE
saveRDS(my_model,file = "youtube.rds")
#3. test model
predicted_dislike <- predict(my_model,newdata = test_df)
predicted_dislike[10:20]
test_df$dislikeCount[10:20]
RMSE <- sqrt(mean((predicted_dislike - test_df$dislikeCount)**2))
library(tuber)
library(tidyverse)
library(caret)
#  app_id = "436728297200-kpvacab71dkrtmh8b2ihc7h7lqv82l4m.apps.googleusercontent.com",
#  app_secret = "zjzwjoARjv4JikRhMr9P0ve6"
#)
#youtube
#content_yt <- get_all_channel_video_stats(
#  channel_id = 'UC5FX2EmL4t7c305hfr-idwQ'
#)
#write.csv(content_yt,file = "cool.csv")
#content_yt
#ดูก่อนว่าได้มากี่ record
nrow(khotkool_youtube_content)
#youtube <- yt_oauth(
#  app_id = "436728297200-kpvacab71dkrtmh8b2ihc7h7lqv82l4m.apps.googleusercontent.com",
#  app_secret = "zjzwjoARjv4JikRhMr9P0ve6"
#)
#youtube
#content_yt <- get_all_channel_video_stats(
#  channel_id = 'UC5FX2EmL4t7c305hfr-idwQ'
#)
khotkool_youtube_content <- read.csv(file = "cool.csv")
#  app_secret = "zjzwjoARjv4JikRhMr9P0ve6"
#)
#youtube
#content_yt <- get_all_channel_video_stats(
#  channel_id = 'UC5FX2EmL4t7c305hfr-idwQ'
#)
#khotkool_youtube_content <- read.csv(file = "cool.csv")
#write.csv(content_yt,file = "cool.csv")
#content_yt
#ดูก่อนว่าได้มากี่ record
nrow(khotkool_youtube_content)
#สั่ง แสดง head ออกมาดู 6 อันว่ามีไรมั่ง
head(khotkool_youtube_content)
#มี feature อะไรให้เราเอามาเล่นบ้างล่ะ
names(khotkool_youtube_content)
glimpse(khotkool_youtube_content)
#เช็คข้อมูลกันก่อนว่ามี missing data ป่าววว
any(is.na(khotkool_youtube_content))
#พอดีเจอ missing data 1 record งั้นลบแม่ง
clean_data_khotkool <- na.omit(khotkool_youtube_content)
#ไหนๆ มีมั้ย เช็คอีกที
any(is.na(clean_data_khotkool))
#comment_content <- get_all_comments(video_id = 'iawt_JNv17g')
#tidyverse pipe จัดการชุดข้อมูลแยกตามรายการที่อยู่ใน โคตรคูล Channel กันทีละอัน
#เบื้องต้นคงเก็บไว้แค่ title viewCount likeCount dislikeCount commentCount url ตัดพวกวันที่ออก ไม่รู้จะเอาไปไร
clean_data_khotkool %>%
filter(str_detect(title,'หมีพาซิ่ง')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 0) -> mee_pa_zine
#clean หมีพาซิ่งแล้ว
head(mee_pa_zine)
#จัดการรายการวันละม้วนต่อ
clean_data_khotkool %>%
filter(str_detect(title,'วันละม้วน')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 1) -> one_la_muan
head(one_la_muan)
clean_data_khotkool %>%
filter(str_detect(title,'คนหน้าหมี') & !str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 2) -> khon_na_mee
head(khon_na_mee)
clean_data_khotkool %>%
filter(str_detect(title,'แม่แยมเอง') & !str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 3) -> mae_yam_eang
head(mae_yam_eang)
clean_data_khotkool %>%
filter(str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 4) -> vlog
head(vlog)
clean_data_khotkool %>%
filter(str_detect(title,'จีบหนูหน่อย') & !str_detect(title,'VLOG')) %>%
select(X,title,viewCount,likeCount,dislikeCount,commentCount,url) %>%
mutate(content_group = 5) -> jeep_nu_noi
head(jeep_nu_noi)
#combine all content channel
all_content_channel <- rbind(mee_pa_zine,one_la_muan,jeep_nu_noi,vlog,mae_yam_eang,khon_na_mee)
#ลำดับรายการที่มียอดวิว มากที่สุดได้แก่รายการรรรรรรร หมีพาซิ่งครับ ซิ่งมียอดวิวมากถึง 30 กว่าล้านยอดวิว
#หมีพาซิ่งมี 18 คลิป
#vlog มี 32 คลิป
#จีบหนูหน่อย 17 คลิป
#คนหน้าหมี 27 คลิป
#แม่แยมเอง 21 คลิป
#วันละม้วน 21 คลิป
all_content_channel %>%
group_by(content_group) %>%
summarise(mostViews = sum(viewCount)) %>%
arrange(desc(mostViews)) %>%
ggplot(.,mapping = aes(
x=reorder(content_group,-mostViews),
y=mostViews/1000)) + geom_col(col = "blue", fill = "navy", alpha = 0.7) +
coord_flip() +
labs(x = "Content Name",
y = "Total Views x 1000")
#งั้นเอารายการหมีพาซิ่งมาดูกันว่าเทปไหน ฮิตสุด
mee_pa_zine %>%
arrange(desc(viewCount)) %>%
head(10) %>%
select(url,viewCount)  %>%
ggplot(.,mapping = aes(
x=reorder(url,-viewCount),
y=viewCount/100)) + geom_col(col = "blue", fill = "navy", alpha = 0.7) +
coord_flip() +
labs(x = "Content Name",
y = "Total Views x 100")
#หาความสัมพันธ์ระหว่าง dislike กับ ยอดวิวว่าสัมพันธ์กันไหม
cor(all_content_channel$dislikeCount,all_content_channel$viewCount)
#หาความสัมพันธ์ระหว่าง dislike กับ ยอดไลค์
cor(all_content_channel$dislikeCount,all_content_channel$likeCount)
#หาความสัมพันธ์ระหว่าง dislike กับ ยอดจำนวนคอมเม้น
cor(all_content_channel$dislikeCount,all_content_channel$commentCount)
#1.prepare data
set.seed(9)
# จำนวน Row
n <- nrow(all_content_channel)
#สุ่ม Data มา 70% จาก data ทั้งหมด
id <- sample(1:n, size = 0.7*n)
#สร้าง train data ที่ได้จาก id
train_df <- all_content_channel[id, ]
#สร้าง test data ที่ไม่อยู่ใน id ด้วย expression -id
test_df <- all_content_channel[-id,]
#ดูโครงสร้างของ data frame
str(all_content_channel)
set.seed(99)
grid_search <- expand.grid(k = c(1,3,5))
ctrl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 5,
verboseIter = T
)
#เอา collumn ที่มีความสัมพันธ์กันมาเทรน
my_model <- train( dislikeCount ~ viewCount + likeCount + commentCount,
data = train_df,
method = "knn",
trControl = ctrl,
tuneGrid = grid_search
)
my_model
#เอา collumn ที่มีความสัมพันธ์กันมาเทรน
my_model <- train( dislikeCount ~ viewCount + likeCount + commentCount + content_group,
data = train_df,
method = "knn",
trControl = ctrl,
tuneGrid = grid_search
)
my_model
#3. test model
predicted_dislike <- predict(my_model,newdata = test_df)
predicted_dislike[10:20]
test_df$dislikeCount[10:20]
RMSE <- sqrt(mean((predicted_dislike - test_df$dislikeCount)**2))
my_model
RMSE
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install_github("geoffjentry/twitteR")
library(tidyverse)
library(devtools)
install_github("geoffjentry/twitteR")
library(twitteR)
install.packages('keras')
library(keras)
mnist <- dataset_mnist()
mnist
mnist <- dataset_mnist()
mnist
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
devtools::install_github("rstudio/keras")
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
library(keras)
mnist <- dataset_mnist()
library(keras)
install_keras()
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
mnist
fashion_mnist <- dataset_fashion_mnist()
library(keras)
fashion_mnist <- dataset_fashion_mnist()
View(fashion_mnist)
library(tidyverse)
library(keras)
fashion_mnist <- dataset_fashion_mnist()
c(train_images, train_labels) %<-% fashion_mnist$train
c(test_images, test_labels) %<-% fashion_mnist$test
View(fashion_mnist)
source('~/Documents/TwinSynergyProjects/MLClass/project/TrainWomen.R')
class_names = c('T-shirt/top',
'Trouser',
'Pullover',
'Dress',
'Coat',
'Sandal',
'Shirt',
'Sneaker',
'Bag',
'Ankle boot')
dim(train_images)
dim(train_labels)
train_labels[1:20]
dim(test_images)
dim(test_labels)
library(ggplot2)
image_1 <- as.data.frame(train_images[1, , ])
colnames(image_1) <- seq_len(ncol(image_1))
image_1$y <- seq_len(nrow(image_1))
image_1 <- gather(image_1, "x", "value", -y)
image_1$x <- as.integer(image_1$x)
ggplot(image_1, aes(x = x, y = y, fill = value)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "black", na.value = NA) +
scale_y_reverse() +
theme_minimal() +
theme(panel.grid = element_blank())   +
theme(aspect.ratio = 1) +
xlab("") +
ylab("")
train_images <- train_images / 255
test_images <- test_images / 255
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) {
img <- train_images[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste(class_names[train_labels[i] + 1]))
}
model <- keras_model_sequential()
model %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
model %>% fit(train_images, train_labels, epochs = 5)
model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
model %>% fit(train_images, train_labels, epochs = 5)
score <- model %>% evaluate(test_images, test_labels)
cat('Test loss:', score$loss, "\n")
cat('Test accuracy:', score$acc, "\n")
predictions <- model %>% predict(test_images)
predictions[1, ]
